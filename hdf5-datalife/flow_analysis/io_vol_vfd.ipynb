{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import traceback\n",
    "\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import math\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 4)\n",
    "# pd.set_option('display.width', 1000)\n",
    "\n",
    "output_path = '../sankey_diagram/dataframe'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Steps\n",
    "1. splits each logs into 3 parts, sim_write , agg_read , agg_write, \n",
    "2. splits each part into 2 dataframe, df_vol and df_vfd \n",
    "3. apply analysis based mapping and merge df_vol and df_vfd\n",
    "4. saved each df_sim_merged, df_agg_read_merged, df_agg_write_merged into parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_empty(df,p=True):\n",
    "    nan_df = df[df.isna().any(axis=1)].copy()\n",
    "    if not nan_df.empty:\n",
    "        if p:\n",
    "            print(f\"NaN rows: {nan_df}\") # check any nan\n",
    "        return list(nan_df.index)\n",
    "    \n",
    "    null_df = df[df.isnull().any(axis=1)].copy()\n",
    "    if not null_df.empty:\n",
    "        if p:\n",
    "            print(f\"NULL rows: {null_df}\") # check any null\n",
    "        return list(null_df.index)\n",
    "    \n",
    "def rec_to_df(records):\n",
    "    df = pd.DataFrame.from_dict(records,orient='index')\n",
    "    df.replace('/mnt/ssd/mtang11/','',regex=True, inplace=True)\n",
    "    df.replace('molecular_dynamics_runs/stage0000/','',regex=True, inplace=True)\n",
    "    \n",
    "    # df['hash_id']= df['hash_id'].astype(str)\n",
    "    # df['hash_id'] = map(lambda x: x.encode('base64','strict'), df['hash_id'])\n",
    "    if 'logical_addr' in df.columns:\n",
    "        df['logical_addr'] = df['logical_addr'].fillna(-1.0) #.astype(int)\n",
    "    if 'access_size' in df.columns:\n",
    "        df['access_size'] = df['access_size'].fillna(0)\n",
    "    #df['logical_addr'] = df['logical_addr'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def df_to_csv(df,file_name,suffix=''):\n",
    "    # df.hash_id=df.hash_id.astype('category').cat.codes\n",
    "    # out_csv=file_name.replace('prov-vfd-','')\n",
    "    out_csv=file_name.replace('.log',f'{suffix}.csv')\n",
    "    out_csv=out_csv.replace('traces','dataframes')\n",
    "\n",
    "    df.to_csv(out_csv,index=False)\n",
    "\n",
    "def df_to_parquet(df,file_name,suffix=''):\n",
    "    out_parquet=file_name.replace('.log',f'{suffix}.parquet')\n",
    "    try:\n",
    "        df.to_parquet(out_parquet, engine='pyarrow') # compression='gzip'\n",
    "        # pyarrow has error with streamlit, downgrade to streamlit==0.84.2\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def print_list_diff(list1, list2):\n",
    "    if list1 == list2:\n",
    "        print(\"same lists\")\n",
    "    else:\n",
    "        print(\"different lists\\nindex:\\t[list1]\\t[list2]\")\n",
    "        for index, (first, second) in enumerate(zip(list1, list2)):\n",
    "            if first != second:\n",
    "                # print(index, first, second)\n",
    "                print(f\"{index}:\\t{first}]\\t[{second}]\")\n",
    "\n",
    "\n",
    "def split_vfd_vol_rec(fname,mode='r'):\n",
    "    vol_r_ops = ['H5VLdataset_read', 'H5VLblob_get', 'H5FD__hermes_read']\n",
    "    vol_w_ops = ['H5VLdataset_write', 'H5VLblob_put', 'H5FD__hermes_write']\n",
    "    # other_ops = ['_get','_create', '_close', '_open',]\n",
    "    other_ops = {'create':'open', 'close':'close', 'open':'open', } # not recording 'get':'get'\n",
    "\n",
    "    vol_rec = {}\n",
    "    vfd_rec = {}\n",
    "    vol_idx = 0\n",
    "    vfd_idx = 0\n",
    "    with open(fname, mode) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = ast.literal_eval(line)\n",
    "                \n",
    "                if any(op in rec['func_name'] for op in vol_r_ops):\n",
    "                    if 'hermes' in rec['func_name']:\n",
    "                        vfd_rec[vfd_idx] = rec\n",
    "                        vfd_rec[vfd_idx]['operation'] = 'read'\n",
    "                        vfd_idx+=1\n",
    "                    else:\n",
    "                        vol_rec[vol_idx] = rec\n",
    "                        vol_rec[vol_idx]['operation'] = 'read'\n",
    "                        vol_idx+=1\n",
    "                elif any(op in rec['func_name'] for op in vol_w_ops):\n",
    "                    if 'hermes' in rec['func_name']:\n",
    "                        vfd_rec[vfd_idx] = rec\n",
    "                        vfd_rec[vfd_idx]['operation'] = 'write'\n",
    "                        vfd_idx+=1\n",
    "                    else:\n",
    "                        vol_rec[vol_idx] = rec\n",
    "                        vol_rec[vol_idx]['operation'] = 'write'\n",
    "                        vol_idx+=1          \n",
    "                # not record other_ops\n",
    "                elif any(op in rec['func_name'] for op in other_ops.keys()):\n",
    "                    _suffix = rec['func_name'].split('_')[-1]\n",
    "                    if 'hermes' in rec['func_name']:\n",
    "                        vfd_rec[vfd_idx] = rec\n",
    "                        vfd_rec[vfd_idx]['operation'] = other_ops[_suffix]\n",
    "                        vfd_idx+=1\n",
    "                    else:\n",
    "                        # modify vol metadata access size to 0\n",
    "                        rec['access_size'] = 0\n",
    "                        vol_rec[vol_idx] = rec\n",
    "                        vol_rec[vol_idx]['operation'] = other_ops[_suffix]\n",
    "                        vol_idx+=1\n",
    "                    \n",
    "            except:\n",
    "                print(\"Erro line:\")\n",
    "                print(line)\n",
    "                # break\n",
    "    return vol_rec, vfd_rec\n",
    "\n",
    "def add_all_op_type(dfvol, dfvfd, vol_map={}, vfd_map={}):\n",
    "    if bool(vol_map) == False:\n",
    "        vol_map={'read':'data', 'write': 'data'}\n",
    "        vfd_map={'H5FD_MEM_DRAW':'data', 'H5FD_MEM_LHEAP': 'lheap'} # not mapto lheap\n",
    "\n",
    "    dfvol['op_type'] = dfvol['operation'].map(vol_map)\n",
    "    dfvol['op_type'] = dfvol['op_type'].fillna('meta')\n",
    "    \n",
    "    dfvfd['mem_type'] = dfvfd['mem_type'].fillna('H5FD_MEM_NTYPES')\n",
    "    dfvfd['op_type'] = dfvfd['mem_type'].map(vfd_map)\n",
    "    dfvfd['op_type'] = dfvfd['op_type'].fillna('meta')\n",
    "\n",
    "def add_vol_op_type(dfvol, vol_map={}):\n",
    "    if bool(vol_map) == False:\n",
    "        vol_map={'read':'data', 'write': 'data'}\n",
    "\n",
    "    dfvol['op_type'] = dfvol['operation'].map(vol_map)\n",
    "    dfvol['op_type'] = dfvol['op_type'].fillna('meta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial VOL Columns: Index(['func_name', 'object_idx', 'time(us)', 'file_name', 'file_no',\n",
      "       'access_size', 'offset', 'logical_addr', 'blob_idx', 'file_intent',\n",
      "       'operation', 'dset_name', 'layout', 'type_size', 'n_elements',\n",
      "       'dimension_cnt', 'dimensions', 'cat', 'op_type', 'data_label'],\n",
      "      dtype='object')\n",
      "Initial VOL Columns: Index(['func_name', 'time(us)', 'file_name', 'file_no', 'file_size',\n",
      "       'file_intent', 'operation', 'object_idx', 'access_size', 'start_addr',\n",
      "       'mem_type', 'cat', 'op_type', 'data_label'],\n",
      "      dtype='object')\n",
      "Final Columns: Index(['data_label_vol', 'object_idx_vol', 'dset_name', 'access_size_vol',\n",
      "       'op_type_vol', 'logical_addr', 'operation_vol', 'n_elements',\n",
      "       'dimension_cnt', 'dimensions', 'file_intent', 'layout', 'time(us)_vol',\n",
      "       'io_idx_vol', 'logical_addr_map', 'data_label_vfd', 'object_idx_vfd',\n",
      "       'access_size_vfd', 'op_type_vfd', 'logical_addr_vfd', 'file_name',\n",
      "       'time(us)_vfd', 'operation_vfd', 'io_idx_vfd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read in simulation data\n",
    "def vol_data_label(df):\n",
    "    data_idx_list = []\n",
    "    dsets = list(set(df['file_no']))\n",
    "    meta_idx = -1\n",
    "    prev_dset = df['dset_name'].iloc[0]\n",
    "    prev_pat = ''\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        _pat = row['op_type']\n",
    "        _dset = row['dset_name']\n",
    "        _str = f\"vol-{_pat}-{_dset}\"\n",
    "        if _pat =='data':\n",
    "            _str = f\"vol-{_pat}-{_dset}-{row['object_idx']}\"\n",
    "        else:\n",
    "            _str = f\"vol-{_pat}-{_dset}{meta_idx}\"\n",
    "            meta_idx-=1\n",
    "\n",
    "        data_idx_list.append(_str)\n",
    "        prev_dset = _dset\n",
    "        prev_pat = _pat\n",
    "\n",
    "    df['data_label'] = data_idx_list\n",
    "    \n",
    "def vfd_data_label_sim_write(df):\n",
    "    data_idx = 0\n",
    "    data_idx_list = []\n",
    "    dsets = list(set(df['file_no']))\n",
    "\n",
    "    curr_dset = dsets.pop(0)\n",
    "    dset_changed = False\n",
    "\n",
    "    meta_idx = 0\n",
    "    prev_pat = ''\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        _pat = row['op_type']\n",
    "        \n",
    "        if _pat =='data':\n",
    "            data_idx_list.append(f\"vfd-{_pat}-{curr_dset}-{data_idx}\")\n",
    "            data_idx+=1\n",
    "        elif index == 0:\n",
    "            # first file object change to match dataset object\n",
    "            data_idx_list.append(f\"vfd-{_pat}-1-1\")\n",
    "        else:\n",
    "            if prev_pat != _pat or row['mem_type'] == 'H5FD_MEM_OHDR':\n",
    "                meta_idx-=1\n",
    "            data_idx_list.append(f\"vfd-{_pat}-{curr_dset}{meta_idx}\")\n",
    "        prev_pat = _pat\n",
    "        \n",
    "        if row['operation'] == 'close':\n",
    "            meta_idx-=1\n",
    "            \n",
    "        if row['file_no'] != curr_dset:\n",
    "            data_idx = 0\n",
    "            curr_dset = dsets.pop(0)\n",
    "        \n",
    "\n",
    "    df['data_label'] = data_idx_list\n",
    "\n",
    "def fill_dset_offset(dfvol, dfvfd, fno2dset_dict={1:'contact_map', 2:'point_cloud'}):\n",
    "    dsets_no = list(fno2dset_dict.keys())\n",
    "    offset_dict = {}\n",
    "    for no in dsets_no:\n",
    "        # find first dataset starting offset\n",
    "        # vfd_sdf[vfd_sdf['data_label'] == f'vfd-data-{dset}-0'].iloc[0]['start_addr']\n",
    "        \n",
    "        idx = dfvfd.data_label.str.match(f'vfd-data-{no}-0').idxmax()\n",
    "        START_OFFSET = dfvfd.iloc[idx]['start_addr']\n",
    "        offset_dict[fno2dset_dict[no]] = START_OFFSET\n",
    "    \n",
    "    dfvol['offset'] = dfvol['dset_name'].map(offset_dict)\n",
    "    return dfvol\n",
    "\n",
    "def hard_code_start_addr(dfvol, dfvfd):\n",
    "    # # hardcode starting address\n",
    "    dfvol = fill_dset_offset(dfvol, dfvfd)\n",
    "    # starting address of blob\n",
    "    idx_vfd = dfvfd.op_type.str.match('data').idxmax() # obtain from first vfd access \n",
    "    BLOB_START_ADDR = dfvfd.iloc[idx_vfd]['access_size'] +  dfvfd.iloc[idx_vfd]['start_addr'] \n",
    "    idx_vol = dfvol.func_name.str.match('H5VLblob_put').idxmax()\n",
    "    dfvol.loc[idx_vol, ['logical_addr']] = BLOB_START_ADDR\n",
    "    # print(dfvol.iloc[idx_vol])\n",
    "    \n",
    "    # starting address of dset\n",
    "    dset_idx = dfvol.index[dfvol['func_name'] == 'H5VLdataset_write'].tolist()\n",
    "    \n",
    "    for dsi in dset_idx:\n",
    "        dfvol.loc[dsi, ['logical_addr']] = dfvol.iloc[dsi]['offset']\n",
    "    \n",
    "    return dfvol\n",
    "\n",
    "def merge_df_sim(dfvol, dfvfd):\n",
    "    df_sim2vol = dfvol[['data_label', 'object_idx','dset_name', 'access_size', 'op_type', 'logical_addr', \n",
    "                        'operation', 'n_elements','dimension_cnt','dimensions','file_intent','layout','time(us)']].copy()\n",
    "    df_sim2vol['io_idx'] = df_sim2vol.index\n",
    "    vol_idx2addr_map = dict(zip(df_sim2vol['data_label'], df_sim2vol['logical_addr']))\n",
    "    df_sim2vol['logical_addr_map'] = df_sim2vol['logical_addr'].astype(int)\n",
    "    \n",
    "    # set vol metadata addr mapping\n",
    "    vol_mod_idx = df_sim2vol.index[df_sim2vol['op_type'] == 'meta'].tolist()\n",
    "    for index in vol_mod_idx:\n",
    "        meta_idx = df_sim2vol['data_label'].iloc[index].split('-')[-1]\n",
    "        meta_idx = 0 - int(meta_idx)\n",
    "        df_sim2vol.loc[index, ['logical_addr_map']] = meta_idx\n",
    "\n",
    "    # print(list(df_sim2vol['logical_addr_map']))\n",
    "\n",
    "    df_vfd2res = dfvfd[['data_label', 'object_idx','access_size', 'op_type','start_addr',\n",
    "                        'file_name', 'time(us)','operation']].copy()\n",
    "    df_vfd2res['io_idx'] = df_vfd2res.index\n",
    "    \n",
    "    # df_vol2res.rename(columns={'start_addr':'logical_addr'}, inplace=True)\n",
    "    df_vfd2res['start_addr'] = df_vfd2res['start_addr'].fillna(0) # or fill -1 for identifying open & close\n",
    "    df_vfd2res['logical_addr_map'] = df_vfd2res['start_addr'].astype(int)\n",
    "    # set vfd metadata addr mapping\n",
    "    vfd_mod_idx = df_vfd2res.index[df_vfd2res['op_type'] == 'meta'].tolist()\n",
    "    for index in vfd_mod_idx:\n",
    "        meta_idx = df_vfd2res['data_label'].iloc[index].split('-')[-1]\n",
    "        meta_idx = 0 - int(meta_idx)\n",
    "        df_vfd2res.loc[index, ['logical_addr_map']] = meta_idx\n",
    "    # print(list(df_vfd2res['logical_addr_map']))\n",
    "    \n",
    "    # modify all meta mapping address and info\n",
    "    df_sim2vol.loc[df_sim2vol['op_type'] == 'meta', 'logical_addr_map'] = 0\n",
    "    df_vfd2res.loc[df_vfd2res['op_type'] == 'meta', 'logical_addr_map'] = 0\n",
    "    \n",
    "    df_vfd2res.rename(columns={'start_addr':'logical_addr_vfd'}, inplace=True)\n",
    "    df_merged = df_sim2vol.merge(df_vfd2res, how='inner', on=['logical_addr_map'], suffixes=['_vol','_vfd'])\n",
    "\n",
    "    # populate file_name\n",
    "    df_merged['file_name'] = df_merged['file_name'].fillna('task0000/residue_100.h5')\n",
    "    df_merged['logical_addr_vfd'] = df_merged['logical_addr_vfd'].astype(int)\n",
    "    # df_merged['object_idx_vol'] = df_merged['object_idx_vol'].fillna(-1)\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].fillna(0)\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].replace(np.nan, 0)\n",
    "    \n",
    "    # correct VOL access size\n",
    "    df_merged.loc[df_merged['operation_vol'] == 'open', 'access_size_vol'] = 0\n",
    "\n",
    "    return df_merged, vol_idx2addr_map\n",
    "\n",
    "fsim='../sankey_diagram/traces/prov-vfd-sim.log'\n",
    "mode='r'\n",
    "\n",
    "vol_sim_rec, vfd_sim_rec = split_vfd_vol_rec(fsim)\n",
    "\n",
    "vol_sdf = rec_to_df(vol_sim_rec)\n",
    "vfd_sdf = rec_to_df(vfd_sim_rec)\n",
    "\n",
    "# add category\n",
    "vol_sdf['cat'] = 'sim-write'\n",
    "vfd_sdf['cat'] = 'sim-write'\n",
    "\n",
    "# TODO: rename, need fix in output format\n",
    "vol_sdf.rename(columns={'io_access_idx':'object_idx'}, inplace=True)\n",
    "vfd_sdf.rename(columns={'io_access_idx':'object_idx'}, inplace=True)\n",
    "\n",
    "# # bfill_cols = ['layout', 'type_size', 'n_elements', 'dimension_cnt', 'dimensions', 'dset_name', 'file_intent']\n",
    "\n",
    "add_all_op_type(vol_sdf,vfd_sdf)\n",
    "# TODO: replace lheap with meta, lheap not use here\n",
    "vfd_sdf['op_type'].replace('lheap','meta',inplace=True)\n",
    "\n",
    "# # add data_label for simulation\n",
    "vol_sdf['dset_name'] = vol_sdf['dset_name'].fillna(method='bfill')\n",
    "vol_sdf['dset_name'] = vol_sdf['dset_name'].fillna(method='ffill')\n",
    "# TODO: modify first point_cloud to contact_map (wrong by bfill)\n",
    "last_cm_idx = vol_sdf.index[vol_sdf['dset_name'] == 'point_cloud'].tolist()[0]\n",
    "vol_sdf.loc[last_cm_idx, ['dset_name']] = 'contact_map'\n",
    "\n",
    "vfd_data_label_sim_write(vfd_sdf)\n",
    "vol_data_label(vol_sdf)\n",
    "vol_sdf.loc[vol_sdf['op_type'] == 'meta', 'object_idx'] = -1\n",
    "# check_empty(vol_sdf)\n",
    "\n",
    "vol_sdf = hard_code_start_addr(vol_sdf,vfd_sdf)\n",
    "\n",
    "# check_empty(vfd_sdf)\n",
    "df_to_csv(vol_sdf,fsim,suffix='-vol')\n",
    "df_to_csv(vfd_sdf,fsim,suffix='-vfd')\n",
    "\n",
    "df_merged, vol_idx2addr_map = merge_df_sim(vol_sdf,vfd_sdf)\n",
    "df_to_csv(df_merged,fsim,suffix='-merged') # output merged to csv\n",
    "\n",
    "# # TODO: use parquet for faster load in later analysis\n",
    "# df_to_parquet(df_merged,fsim,suffix='-merged')\n",
    "df_to_csv(df_merged,fsim,suffix='-merged')\n",
    "\n",
    "# print(df_merged.loc[df_merged['op_type_vfd'] == 'meta'])\n",
    "\n",
    "print(f\"Initial VOL Columns: {vol_sdf.columns}\")\n",
    "print(f\"Initial VOL Columns: {vfd_sdf.columns}\")\n",
    "print(f\"Final Columns: {df_merged.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial VOL Columns: Index(['index', 'func_name', 'object_idx', 'time(us)', 'file_name', 'file_no',\n",
      "       'access_size', 'offset', 'logical_addr', 'blob_idx', 'file_intent',\n",
      "       'operation', 'dset_name', 'layout', 'type_size', 'n_elements',\n",
      "       'dimension_cnt', 'dimensions', 'cat', 'op_type', 'data_label'],\n",
      "      dtype='object')\n",
      "Initial VOL Columns: Index(['index', 'func_name', 'time(us)', 'file_name', 'file_no', 'file_size',\n",
      "       'file_intent', 'operation', 'object_idx', 'access_size', 'start_addr',\n",
      "       'mem_type', 'cat', 'next_addr', 'data_label', 'op_type'],\n",
      "      dtype='object')\n",
      "Final Columns: Index(['data_label_vol', 'object_idx_vol', 'dset_name', 'access_size_vol',\n",
      "       'op_type_vol', 'logical_addr_vol', 'operation_vol', 'n_elements',\n",
      "       'dimension_cnt', 'dimensions', 'file_intent', 'layout', 'time(us)_vol',\n",
      "       'io_idx_vol', 'logical_addr_map', 'data_label_vfd', 'object_idx_vfd',\n",
      "       'access_size_vfd', 'op_type_vfd', 'next_addr', 'logical_addr_vfd',\n",
      "       'file_name', 'time(us)_vfd', 'operation_vfd', 'io_idx_vfd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## map only aggregation read\n",
    "\n",
    "# read in aggregation data\n",
    "def vfd_op_data_label_agg_read(df, dset_offsets):\n",
    "\n",
    "    data_label_list = []\n",
    " \n",
    "    curr_dset = 1\n",
    "    next_offset = dset_offsets[curr_dset]\n",
    "    dset_changed = False\n",
    "    \n",
    "    data_idx = 1\n",
    "    last_addr = -1 # vfd has no -1 address\n",
    "    pattern_list = []\n",
    "\n",
    "    meta_idx = 0\n",
    "    prev_pat=''\n",
    "    \n",
    "    for index, row in df.iterrows(): #islice(df.iterrows(), start_idx, None):\n",
    "        if row['mem_type'] == \"H5FD_MEM_DRAW\":\n",
    "            if row['access_size'] == 4096:\n",
    "                last_addr = row['next_addr']\n",
    "                data_label_list.append(f'vfd-data-{curr_dset}-{data_idx}')\n",
    "                pattern_list.append('loc')\n",
    "            elif row['start_addr'] == last_addr:\n",
    "                data_label_list.append(f'vfd-data-{curr_dset}-{data_idx}')\n",
    "                pattern_list.append('data')\n",
    "                data_idx+=1\n",
    "                last_addr = 0\n",
    "            else:\n",
    "                data_label_list.append(f'vfd-data-{curr_dset}-{0}') # first access\n",
    "                pattern_list.append('data')\n",
    "            prev_pat = 'data'\n",
    "        else:\n",
    "            if prev_pat != 'meta' or row['mem_type'] == 'H5FD_MEM_OHDR':\n",
    "                meta_idx-=1\n",
    "            \n",
    "            data_label_list.append(f'vfd-meta{meta_idx}') #-{curr_dset}\n",
    "            last_addr = row['next_addr']\n",
    "            pattern_list.append('meta')\n",
    "        \n",
    "            prev_pat = 'meta'\n",
    "        \n",
    "        # detect dataset change\n",
    "        if row['start_addr'] == next_offset:\n",
    "            data_idx = 0\n",
    "            curr_dset+=1\n",
    "            if len(dset_offsets) <= curr_dset:\n",
    "                next_offset = 0\n",
    "            else:\n",
    "                next_offset = dset_offsets[curr_dset]\n",
    "            dset_changed = True\n",
    "    \n",
    "    data_label_list[-1] = f'vfd-meta{meta_idx}' # last idx for point_cloud\n",
    "    # f'vfd-meta-{curr_dset}{meta_idx}'\n",
    "    \n",
    "    df['data_label'] = data_label_list\n",
    "    df['op_type'] = pattern_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_df_agg_read(dfvol, dfvfd,vol_idx2addr_map):\n",
    "    df_agg2vol = dfvol[['data_label', 'object_idx','dset_name', 'access_size', 'op_type','logical_addr',\n",
    "                        'operation', 'n_elements','dimension_cnt','dimensions','file_intent','layout','time(us)']].copy()\n",
    "    df_agg2vol['io_idx'] = df_agg2vol.index\n",
    "    df_vfd2res = dfvfd[['data_label', 'object_idx','access_size', 'op_type', 'next_addr','start_addr',\n",
    "                        'file_name','time(us)','operation']].copy()\n",
    "    df_vfd2res['io_idx'] = df_vfd2res.index\n",
    "\n",
    "    # df_agg2vol['logical_addr'] = df_agg2vol['logical_addr'].astype(int)\n",
    "    \n",
    "    # address map for vol\n",
    "    df_agg2vol['logical_addr_map'] = df_agg2vol['data_label'].map(vol_idx2addr_map)\n",
    "    # set vol metadata addr mapping\n",
    "    vol_mod_idx = df_agg2vol.index[df_agg2vol['op_type'] == 'meta'].tolist()\n",
    "    for index in vol_mod_idx:\n",
    "        meta_idx = df_agg2vol['data_label'].iloc[index].split('-')[-1]\n",
    "        meta_idx = 0 - int(meta_idx)\n",
    "        df_agg2vol.loc[index, ['logical_addr_map']] = meta_idx\n",
    "\n",
    "    # df_agg2vol.loc[df_agg2vol['op_type'] == 'meta', 'logical_addr_map'] = 0 # all meta map to 0\n",
    "    \n",
    "    # split by op_type\n",
    "    df_vfd2res_meta = df_vfd2res[df_vfd2res['op_type'] == 'meta'].copy() # use 0 for meta\n",
    "    df_vfd2res_loc = df_vfd2res[df_vfd2res['op_type'] == 'loc'].copy() # this match vol with start_addr\n",
    "    df_vfd2res_data = df_vfd2res[df_vfd2res['op_type'] == 'data'].copy() # this match loc with next_addr\n",
    "    \n",
    "    # address map for vfd-meta\n",
    "    # df_vfd2res_meta['logical_addr_map'] = 0\n",
    "    df_vfd2res_loc['logical_addr_map'] = df_vfd2res_loc['start_addr']\n",
    "    # set vfd metadata addr mapping\n",
    "    vfd_mod_idx =  list(df_vfd2res_meta.index)#df_vfd2res_meta.index[df_vfd2res_meta['op_type'] == 'meta'].tolist()\n",
    "    for index in vfd_mod_idx:\n",
    "        meta_idx = df_vfd2res['data_label'].iloc[index].split('-')[-1]\n",
    "        meta_idx = 0 - int(meta_idx)\n",
    "        df_vfd2res_meta.loc[index, ['logical_addr_map']] = meta_idx\n",
    "    \n",
    "    # address map for vfd-data\n",
    "    df_vfd2res_data_map  = dict(zip(df_vfd2res_loc['next_addr'], df_vfd2res_loc['start_addr']))\n",
    "    df_vfd2res_data['logical_addr_map'] = df_vfd2res_data['start_addr'].map(df_vfd2res_data_map)\n",
    "    \n",
    "    nan_idx = check_empty(df_vfd2res_data,p=False)\n",
    "    for idx in nan_idx:\n",
    "        df_vfd2res_data.loc[idx, ['logical_addr_map']] = df_vfd2res.iloc[idx]['start_addr']\n",
    "    \n",
    "    # # rename and conver types\n",
    "    # df_vol2res_meta['logical_addr_map'] = df_vol2res_meta['start_addr']\n",
    "    # df_vol2res_loc['logical_addr_map'] = df_vol2res_loc['start_addr']\n",
    "    \n",
    "    df_vfd2res = pd.concat([df_vfd2res_meta,df_vfd2res_loc, df_vfd2res_data], axis=0) #.set_index('object_idx')\n",
    "\n",
    "    # # df_vol2res_data['logical_addr_map'] = df_vol2res_data['logical_addr_map'].astype(int)\n",
    "    df_vfd2res.rename(columns={'start_addr':'logical_addr'}, inplace=True)\n",
    "    # df_vfd2res['logical_addr'] = df_vfd2res['logical_addr'].replace(-1,0) # all -1 address to 0\n",
    "    \n",
    "    df_merged = df_agg2vol.merge(df_vfd2res, how='inner', on=['logical_addr_map'], suffixes=['_vol','_vfd'])\n",
    "    df_merged['logical_addr_vfd'] = df_merged['logical_addr_vfd'].astype(int)\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].fillna(0)\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].replace(np.nan, 0)\n",
    "\n",
    "    # correct vfd addr and VOL access size\n",
    "    df_merged.loc[df_merged['logical_addr_vfd'] == -1, 'logical_addr_vfd'] = 0\n",
    "    df_merged.loc[df_merged['operation_vol'] == 'open', 'access_size_vol'] = 0\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "fagg='../sankey_diagram/traces/prov-vfd-agg.log'\n",
    "\n",
    "vol_agg_rec, vfd_agg_rec = split_vfd_vol_rec(fagg)\n",
    "\n",
    "vol_adf = rec_to_df(vol_agg_rec)\n",
    "vfd_adf = rec_to_df(vfd_agg_rec)\n",
    "\n",
    "\n",
    "# add category, split read write based on filename\n",
    "vol_adf['cat'] = np.where(vol_adf['file_name']== 'aggregate.h5', 'agg-write', 'agg-read')\n",
    "vfd_adf['cat'] = np.where(vfd_adf['file_name']== 'aggregate.h5', 'agg-write', 'agg-read')\n",
    "\n",
    "# add columns for mapping\n",
    "vfd_adf['start_addr'] = vfd_adf['start_addr'].fillna(-1.0)\n",
    "vfd_adf['access_size'] = vfd_adf['access_size'].fillna(0.0)\n",
    "vfd_adf['next_addr'] = vfd_adf['start_addr'] + vfd_adf['access_size']\n",
    "vfd_adf['next_addr'] = vfd_adf['next_addr'].astype(int)\n",
    "\n",
    "# add mapping for data_label\n",
    "vol_adf_read = vol_adf[vol_adf['cat'] == 'agg-read'].copy().reset_index()\n",
    "vfd_adf_read = vfd_adf[vfd_adf['cat'] == 'agg-read'].copy().reset_index()\n",
    "# TODO: rename, need fix in output format\n",
    "vol_adf_read.rename(columns={'io_access_idx':'object_idx'}, inplace=True)\n",
    "vfd_adf_read.rename(columns={'io_access_idx':'object_idx'}, inplace=True)\n",
    "\n",
    "# get dset offsets from vol df\n",
    "dset_offsets = list(set(vol_adf_read[vol_adf_read['func_name'] == 'H5VLdataset_read']['offset']))\n",
    "# print(dset_offsets)\n",
    "\n",
    "# # add data_label for vol\n",
    "vol_adf_read['dset_name'] = vol_adf_read['dset_name'].fillna(method='bfill')\n",
    "vol_adf_read['dset_name'] = vol_adf_read['dset_name'].fillna(method='ffill')\n",
    "add_vol_op_type(vol_adf_read)\n",
    "vol_data_label(vol_adf_read)\n",
    "vol_adf_read.loc[vol_adf_read['op_type'] == 'meta', 'object_idx'] = -1\n",
    "\n",
    "## add data_label for vfd\n",
    "vfd_adf_read = vfd_op_data_label_agg_read(vfd_adf_read,dset_offsets)\n",
    "# # df_to_csv(vfd_adf_read,fagg,suffix='-vfd-read')\n",
    "\n",
    "\n",
    "df_to_csv(vol_adf_read,fagg,suffix='-vol-read')\n",
    "df_to_csv(vfd_adf_read,fagg,suffix='-vfd-read')\n",
    "\n",
    "df_agg_read_merged = merge_df_agg_read(vol_adf_read,vfd_adf_read,vol_idx2addr_map)\n",
    "\n",
    "df_to_csv(df_agg_read_merged,fagg,suffix='-read-merged')\n",
    "# df_to_parquet(df_agg_read_merged,fagg,suffix='-read-merged')\n",
    "\n",
    "# print(df_agg_read_merged.loc[df_agg_read_merged['op_type_vfd'] == 'meta'])\n",
    "\n",
    "print(f\"Initial VOL Columns: {vol_adf_read.columns}\")\n",
    "print(f\"Initial VOL Columns: {vfd_adf_read.columns}\")\n",
    "print(f\"Final Columns: {df_agg_read_merged.columns}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create lables for AGG_WRITE phase\n",
    "1. create op_type for all 16B, 3KB I/O, and metadata and H5FD_MEM_LHEAP with map \n",
    "``` \n",
    "op_type_map = { # for notes only now\n",
    "    'mem_type' : {\n",
    "        'H5FD_MEM_DRAW' : { 'access_size':{ 16 : 'loc', 'others' : 'data'} },\n",
    "        'H5FD_MEM_LHEAP' : 'lheap','H5FD_MEM_OHDR': 'meta','H5FD_MEM_SUPER': 'meta'}\n",
    "}\n",
    "```\n",
    "2. create data_label for all H5FD_MEM_DRAW that are not 16B (location data)\n",
    "3. map data_label with start_addr-3KB to end_addr-16B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC_IO_SIZE = 2400.0\n",
      "PC_START_ADDR = 37268781.0\n",
      "Initial VOL Columns: Index(['index', 'func_name', 'object_idx', 'time(us)', 'file_name', 'file_no',\n",
      "       'access_size', 'offset', 'logical_addr', 'blob_idx', 'file_intent',\n",
      "       'operation', 'dset_name', 'layout', 'type_size', 'n_elements',\n",
      "       'dimension_cnt', 'dimensions', 'cat', 'op_type', 'data_label'],\n",
      "      dtype='object')\n",
      "Initial VOL Columns: Index(['index', 'func_name', 'time(us)', 'file_name', 'file_no', 'file_size',\n",
      "       'file_intent', 'operation', 'object_idx', 'access_size', 'start_addr',\n",
      "       'mem_type', 'cat', 'next_addr', 'op_type', 'data_label'],\n",
      "      dtype='object')\n",
      "Final Columns: Index(['data_label_vol', 'object_idx_vol', 'dset_name', 'access_size_vol',\n",
      "       'logical_addr_vol', 'op_type_vol', 'operation_vol', 'n_elements',\n",
      "       'dimension_cnt', 'dimensions', 'file_intent', 'layout', 'time(us)_vol',\n",
      "       'io_idx_vol', 'logical_addr_map', 'data_label_vfd', 'object_idx_vfd',\n",
      "       'access_size_vfd', 'logical_addr_vfd', 'next_addr', 'op_type_vfd',\n",
      "       'file_name', 'time(us)_vfd', 'operation_vfd', 'io_idx_vfd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# map only write\n",
    "\n",
    "def vfd_op_type_agg_write(df):\n",
    "    op_type_list = []\n",
    "    # tmp_row = pd.DataFrame(columns=list(df.columns))\n",
    "    for index, row in df.iterrows():\n",
    "        if row['mem_type'] == 'H5FD_MEM_DRAW':\n",
    "            if row['access_size'] == 16:\n",
    "                op_type_list.append('loc')\n",
    "            else:\n",
    "                op_type_list.append('data')\n",
    "        elif row['mem_type'] == 'H5FD_MEM_LHEAP':\n",
    "            op_type_list.append('lheap')\n",
    "        else:\n",
    "            op_type_list.append('meta')\n",
    "    \n",
    "    df['op_type'] = op_type_list\n",
    "    return df\n",
    "\n",
    "        \n",
    "def vfd_data_label_agg_write(df, PC_IO_SIZE):\n",
    "    # TODO: needs improvement\n",
    "\n",
    "    data_idx = 0\n",
    "    pc_idx = 0\n",
    "    lheap_idx = 0\n",
    "\n",
    "    meta_idx = 0\n",
    "    prev_pat = ''\n",
    "        \n",
    "    data_label_list = []\n",
    "    loc_map = {}\n",
    "    data_label_map  = {}\n",
    "    \n",
    "    first_chunk_index = 0 # record index at dataframe\n",
    "    prev_address = df[df['op_type'] == 'data'].iloc[0]['start_addr'] # first data address\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        _pat = row['op_type']\n",
    "        if _pat == 'data':\n",
    "            if row['access_size'] == PC_IO_SIZE:\n",
    "                data_label_list.append(f'vfd-data-point_cloud-{pc_idx}')\n",
    "                pc_idx +=1\n",
    "            else:\n",
    "                _str = f'vfd-data-contact_map-{data_idx}'\n",
    "                data_label_list.append(_str)\n",
    "                if first_chunk_index == 0:\n",
    "                    if row['start_addr'] != prev_address:\n",
    "                        first_chunk_index = index\n",
    "                else:\n",
    "                    data_label_map[row['start_addr']] = _str\n",
    "                data_idx +=1\n",
    "                prev_address = row['next_addr']\n",
    "        elif _pat == 'lheap':\n",
    "            data_label_list.append(f'vfd-{_pat}-1-{lheap_idx}')\n",
    "            lheap_idx+=1     \n",
    "        elif _pat == 'meta':\n",
    "            if prev_pat != 'meta' or index == 1: # after file open\n",
    "                #or row['mem_type'] == 'H5FD_MEM_OHDR': # index == 1\n",
    "                meta_idx-=1\n",
    "            data_label_list.append(f'vfd-{_pat}{meta_idx}')\n",
    "\n",
    "        else: \n",
    "            if loc_map:\n",
    "                data_label_list.append('0')\n",
    "            else:\n",
    "                data_label_list.append(f'vfd-lheap-1')\n",
    "            loc_map[row['next_addr']] = int(row['object_idx'])\n",
    "        \n",
    "        prev_pat = _pat\n",
    "    \n",
    "    # map some 16B with data_label\n",
    "    # for k in data_label_map.keys():\n",
    "    #     print(f\" {data_label_list[loc_map[k]]} = {data_label_map[k]}\")\n",
    "    \n",
    "    #find and map the rest 16B with initial chunk\n",
    "    indices = [i for i, x in enumerate(data_label_list) if x == '0']\n",
    "    \n",
    "    data_idx = 0\n",
    "    for idx in indices:\n",
    "        data_label_list[idx] = f'vfd-data-contact_map-{data_idx}'\n",
    "        data_idx+=1\n",
    "\n",
    "    df['data_label'] = data_label_list\n",
    "\n",
    "    # print(df.loc[df['op_type'] == 'lheap'])\n",
    "\n",
    "def get_point_cloud_io_size(dfvol):\n",
    "    # idx = dfvol.dset_name.str.match('point_cloud').idxmax()\n",
    "    idx = dfvol.data_label.str.match(f'vol-data-point_cloud-0').idxmax()\n",
    "    # dims = ast.literal_eval(lastrow['dimensions'])\n",
    "    first_dim = dfvol.iloc[idx]['dimensions'][0] #lastrow['dimensions'][idx]\n",
    "    access_size = dfvol.iloc[idx]['access_size']\n",
    "    io_size = access_size / first_dim\n",
    "    return io_size\n",
    "\n",
    "def hard_code_start_addr_agg(dfvol, dfvfd):\n",
    "    # # hardcode starting address\n",
    "    for dset in ['contact_map', 'point_cloud']:\n",
    "        idx_vfd = dfvfd.data_label.str.match(f'vfd-data-{dset}-0').idxmax()\n",
    "        START_ADDR = dfvfd.iloc[idx_vfd]['start_addr']\n",
    "        \n",
    "        idx_vol = dfvol.data_label.str.match(f'vol-data-{dset}-0').idxmax()\n",
    "        dfvol.loc[idx_vol, ['logical_addr']] = START_ADDR\n",
    "\n",
    "        idx_vol = dfvol.data_label.str.match(f'vol-data-{dset}-1').idxmax()\n",
    "        if idx_vol:\n",
    "            dfvol.loc[idx_vol, ['logical_addr']] = START_ADDR\n",
    "    \n",
    "    return dfvol\n",
    "\n",
    "def get_pc_start_addr(dfvfd):\n",
    "    idx_ls_pc = dfvfd.index[dfvfd['access_size'] == PC_IO_SIZE].tolist()[0]\n",
    "    start_addr = dfvfd.iloc[idx_ls_pc]['start_addr']\n",
    "    return start_addr\n",
    "\n",
    "def merge_df_agg_write(dfvol, dfvfd):\n",
    "    df_agg2vol = dfvol[['data_label', 'object_idx','dset_name', 'access_size','logical_addr','op_type', \n",
    "                        'operation', 'n_elements','dimension_cnt','dimensions','file_intent','layout','time(us)']].copy()\n",
    "    df_agg2vol['io_idx'] = df_agg2vol.index\n",
    "    df_vol2vfd = dfvfd[['data_label', 'object_idx', 'access_size', 'start_addr', 'next_addr','op_type',\n",
    "                        'file_name', 'time(us)','operation']].copy()\n",
    "    df_vol2vfd['io_idx'] = df_vol2vfd.index\n",
    "    \n",
    "    # vol map # set 0 for meta\n",
    "    df_agg2vol['logical_addr_map'] = df_agg2vol['logical_addr'].astype(int)\n",
    "    # set vol metadata addr mapping\n",
    "    vol_mod_idx = df_agg2vol.index[df_agg2vol['op_type'] == 'meta'].tolist()\n",
    "    for index in vol_mod_idx:\n",
    "        meta_idx = df_agg2vol['data_label'].iloc[index].split('-')[-1]\n",
    "        meta_idx = 0 - int(meta_idx)\n",
    "        df_agg2vol.loc[index, ['logical_addr_map']] = meta_idx\n",
    "    # df_agg2vol.loc[df_agg2vol['op_type'].str.match('meta'), 'logical_addr_map'] = 0\n",
    "    \n",
    "    # data map\n",
    "    df_vfd2res_data = df_vol2vfd[df_vol2vfd['op_type'] == 'data'].copy()\n",
    "    df_vfd2res_data['logical_addr_map'] = df_vfd2res_data['start_addr'].astype(int)\n",
    "\n",
    "    # modify point_cloud to use only 1 address for mapping \n",
    "    # print(df_agg2vol[df_agg2vol['dset_name'] == 'point_cloud'])\n",
    "    # df_agg2vol.loc[(df_agg2vol['dset_name'] == 'point_cloud') & (df_agg2vol['dset_name'] == 'point_cloud'), 'logical_addr_vfd'] = 0\n",
    "    df_vfd2res_data.loc[df_vfd2res_data['data_label'].str.contains('point_cloud'), 'logical_addr_map'] = PC_START_ADDR\n",
    "\n",
    "    # print(PC_START_ADDR)\n",
    "    # pd.set_option('display.max_rows', None)\n",
    "    # print(df_vfd2res_data.tail(100))\n",
    "    \n",
    "    # loc maps by data_label \n",
    "    df_vfd2res_loc = df_vol2vfd[df_vol2vfd['op_type'] == 'loc'].copy()\n",
    "    vfd2res_loc_map  = dict(zip(df_vfd2res_data['data_label'], df_vfd2res_data['start_addr']))\n",
    "    df_vfd2res_loc['logical_addr_map'] = df_vfd2res_loc['data_label'].map(vfd2res_loc_map)\n",
    "    \n",
    "    # lheap maps by address\n",
    "    df_vfd2res_lheap = df_vol2vfd[df_vol2vfd['op_type'] == 'lheap'].copy()\n",
    "    vfd2res_lheap_map = dict(zip(df_vfd2res_data['start_addr'], df_vfd2res_data['next_addr']))\n",
    "    df_vfd2res_lheap['logical_addr_map'] = df_vfd2res_lheap['next_addr'].map(vfd2res_lheap_map)\n",
    "    \n",
    "    # meta maps to 0\n",
    "    df_vfd2res_meta = df_vol2vfd[df_vol2vfd['op_type'] == 'meta'].copy()\n",
    "    vfd_mod_idx =  list(df_vfd2res_meta.index)#df_vfd2res_meta.index[df_vfd2res_meta['op_type'] == 'meta'].tolist()\n",
    "    for index in vfd_mod_idx:\n",
    "        meta_idx = df_vol2vfd['data_label'].iloc[index].split('-')[-1]\n",
    "        meta_idx = 0 - int(meta_idx)\n",
    "        df_vfd2res_meta.loc[index, ['logical_addr_map']] = meta_idx\n",
    "    # df_vfd2res_meta['logical_addr_map'] = 0\n",
    "    \n",
    "    df_vfd2res = pd.concat([df_vfd2res_data,df_vfd2res_loc,df_vfd2res_lheap,df_vfd2res_meta], axis=0) #.set_index('object_idx')\n",
    "    df_vfd2res = df_vfd2res.rename(columns={'start_addr':'logical_addr'})\n",
    "    df_merged = df_agg2vol.merge(df_vfd2res, how='inner', on=['logical_addr_map'], suffixes=['_vol','_vfd'])\n",
    "\n",
    "    # populate file_name\n",
    "    df_merged['file_name'] = df_merged['file_name'].fillna('aggregate.h5')\n",
    "    df_merged['logical_addr_vfd'] = df_merged['logical_addr_vfd'].astype(int)\n",
    "    # df_merged['logical_addr_vfd'] = df_merged['logical_addr_vfd'].replace(-1,0) # all -1 address to 0\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].fillna(0)\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].replace(np.nan, 0)\n",
    "    \n",
    "\n",
    "    # correct vfd addr and VOL access size\n",
    "    df_merged.loc[df_merged['logical_addr_vfd'] == -1, 'logical_addr_vfd'] = 0\n",
    "    df_merged.loc[df_merged['operation_vol'] == 'open', 'access_size_vol'] = 0\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "vol_adf_write = vol_adf[vol_adf['cat'] == 'agg-write'].copy().reset_index()\n",
    "vfd_adf_write = vfd_adf[vfd_adf['cat'] == 'agg-write'].copy().reset_index()\n",
    "# TODO: rename, need fix in output format\n",
    "vol_adf_write.rename(columns={'io_access_idx':'object_idx'}, inplace=True)\n",
    "vfd_adf_write.rename(columns={'io_access_idx':'object_idx'}, inplace=True)\n",
    "\n",
    "# check_empty(vol_adf_write)\n",
    "\n",
    "# add data_label for vol\n",
    "vol_adf_write['dset_name'] = vol_adf_write['dset_name'].fillna(method='bfill')\n",
    "vol_adf_write['dset_name'] = vol_adf_write['dset_name'].fillna(method='ffill')\n",
    "add_vol_op_type(vol_adf_write)\n",
    "vol_data_label(vol_adf_write)\n",
    "vol_adf_write.loc[vol_adf_write['op_type'] == 'meta', 'object_idx'] = -1\n",
    "\n",
    "vfd_adf_write = vfd_op_type_agg_write(vfd_adf_write)\n",
    "PC_IO_SIZE = get_point_cloud_io_size(vol_adf_write)\n",
    "print(f\"PC_IO_SIZE = {PC_IO_SIZE}\")\n",
    "vfd_data_label_agg_write(vfd_adf_write, PC_IO_SIZE)\n",
    "\n",
    "# modify vol logical address for mapping\n",
    "# 37268781 \n",
    "PC_START_ADDR = get_pc_start_addr(vfd_adf_write)\n",
    "print(f\"PC_START_ADDR = {PC_START_ADDR}\")\n",
    "\n",
    "# get logical address from vfd to vol\n",
    "vol_adf_write = hard_code_start_addr_agg(vol_adf_write,vfd_adf_write)\n",
    "\n",
    "df_to_csv(vol_adf_write,fagg,suffix='-vol-write')\n",
    "df_to_csv(vfd_adf_write,fagg,suffix='-vfd-write')\n",
    "# df_to_parquet(vol_adf_write,fagg,suffix='-vol-write')\n",
    "# df_to_parquet(vfd_adf_write,fagg,suffix='-vfd-write')\n",
    "\n",
    "df_agg_write_merged = merge_df_agg_write(vol_adf_write,vfd_adf_write)\n",
    "\n",
    "df_to_csv(df_agg_write_merged,fagg,suffix='-write-merged')\n",
    "# df_to_parquet(df_agg_write_merged,fagg,suffix='-write-merged')\n",
    "\n",
    "print(f\"Initial VOL Columns: {vol_adf_write.columns}\")\n",
    "print(f\"Initial VOL Columns: {vfd_adf_write.columns}\")\n",
    "print(f\"Final Columns: {df_agg_write_merged.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
