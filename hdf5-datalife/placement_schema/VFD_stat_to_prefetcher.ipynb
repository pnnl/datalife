{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# reading input log file\n",
    "# import ruamel.yaml\n",
    "import yaml\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import traceback\n",
    "from csv import excel\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "TRACE_PARALLEL=False\n",
    "PROCS=12\n",
    "\n",
    "test_name = \"summer_sam_seq12f9s\"\n",
    "# 12p12f9s \n",
    "# summer_sam_seq6f9s summer_sam_seq12f9s summer_sam_seq12f9s-fb\n",
    "# seq9f9s 1p9f9s_run seq6f3s seq12f9s seq24f9s\n",
    "# ddmd_12p4s_short ddmd_12p4s\n",
    "\n",
    "\n",
    "stat_path=f\"../example_stat/{test_name}\"\n",
    "image_path=f\"{stat_path}/images\"\n",
    "\n",
    "\n",
    "PREFETCH_SCHEMA = True # True False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO for PREFETCH_SCHEMA\n",
    "- VOL and VFD logs uses full file path, only in networkx nodes remove path name\n",
    "- donot demote blob if file is still used in later stage\n",
    "    - currently only do promote blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ORDER_LIST = {\n",
    "    \"run_idfeature\": 0,\n",
    "    \"run_tracksingle\": 1,\n",
    "    \"run_gettracks\": 2,\n",
    "    \"run_trackstats\": 3,\n",
    "    \"run_identifymcs\": 4,\n",
    "    \"run_matchpf\": 5,\n",
    "    \"run_mcsstats\": 6,\n",
    "    \"run_robustmcs\": 7,\n",
    "    \"run_mapfeature\": 8,\n",
    "    \"run_speed\": 9\n",
    "}\n",
    "\n",
    "# TASK_ORDER_LIST = {\n",
    "#     \"openmm-0000\": 0,\"openmm-0001\": 0,\"openmm-0002\": 0,\n",
    "#     \"openmm-0003\": 0,\"openmm-0004\": 0,\"openmm-0005\": 0,\n",
    "#     \"openmm-0006\": 0,\"openmm-0007\": 0,\"openmm-0008\": 0,\n",
    "#     \"openmm-0009\": 0,\"openmm-0010\": 0,\"openmm-0011\": 0,\n",
    "#     \"aggregate\": 1,\n",
    "#     \"train\": 2,\n",
    "#     \"inference\": 3,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../example_stat/summer_sam_seq12f9s/170202_vfd-data-stat.yaml']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def search_files_with_name(directory, pattern):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if re.search(pattern, file) and ('.yaml' in file or '.yml' in file):\n",
    "                file_list.append(os.path.join(root, file))\n",
    "                #print(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n",
    "vfd_files = search_files_with_name(stat_path, \"vfd\")\n",
    "# vfd_files = vfd_files[0:1]\n",
    "print(vfd_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../example_stat/summer_sam_seq12f9s/170202_vfd-data-stat.yaml\n",
      "dict_keys(['../example_stat/summer_sam_seq12f9s/170202_vfd-data-stat.yaml'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_vfd_yaml(vfd_files):\n",
    "    # loag into {file_name:yaml_data} format\n",
    "    ret_dict = {}\n",
    "    tmp_dict = {}\n",
    "    for f in vfd_files:\n",
    "        if '.yaml' in f or '.yml' in f:\n",
    "            with open(f, \"r\") as stream:\n",
    "                print(f\"loading {f}\")\n",
    "                \n",
    "                try:\n",
    "                    tmp_dict = yaml.safe_load(stream)\n",
    "                    ret_dict[f] = tmp_dict\n",
    "                    # print(tmp_dict)\n",
    "                except yaml.YAMLError as exc:\n",
    "                    print(exc)\n",
    "                    print(\"Error loading yaml file\")\n",
    "                    exit(1)\n",
    "    return ret_dict\n",
    "\n",
    "vfd_file_dict = load_vfd_yaml(vfd_files)\n",
    "print(vfd_file_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total overhead: 5457.0 ms\n"
     ]
    }
   ],
   "source": [
    "def show_all_overhead(vfd_file_dict):\n",
    "    overhead = 0\n",
    "    for pid_file, pid_stat in vfd_file_dict.items():\n",
    "        for item in pid_stat:\n",
    "            if \"Task\" in item.keys():\n",
    "                overhead += float(item['VFD-Total-Overhead(ms)'])\n",
    "                # print(item['VFD-Total-Overhead(ms)'])\n",
    "                # print(item[\"Task\"])\n",
    "                # for item in item[\"Task\"]:\n",
    "    print(f\"Total overhead: {overhead} ms\")\n",
    "\n",
    "show_all_overhead(vfd_file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pid_from_file_path(file_path):\n",
    "    # Define the regular expression pattern to find numbers in the file path\n",
    "    pattern = r'\\d+'\n",
    "    # Use the re.findall function to find all occurrences of the pattern in the file path\n",
    "    numbers = re.findall(pattern, file_path)\n",
    "    # Since there might be multiple numbers in the file path, you can extract the last one\n",
    "    if numbers:\n",
    "        desired_number = int(numbers[-1])\n",
    "        print(\"task_pid:\", desired_number)\n",
    "    else:\n",
    "        print(\"No task_pid found in the file path.\")\n",
    "    return desired_number\n",
    "\n",
    "def get_op_blob_range(h5fd_stat):\n",
    "    # Format of io_stat\n",
    "    '''\n",
    "    read_bytes: 0\n",
    "        read_cnt: 0\n",
    "        read_ranges: {1:[0, 0]}\n",
    "        write_bytes: 7746756\n",
    "        write_cnt: 52\n",
    "        write_ranges:{2:[0, 0]}\n",
    "    '''\n",
    "    read_ranges = h5fd_stat['read_ranges']\n",
    "    write_ranges = h5fd_stat['write_ranges']\n",
    "    \n",
    "    if read_ranges:\n",
    "        start_op_idx = min(read_ranges.keys())\n",
    "        end_op_idx = max(read_ranges.keys())\n",
    "        start_blob = min(elem[0] for elem in list(read_ranges.values())) #min(read_ranges.values())\n",
    "        end_blob = max(elem[1] for elem in list(read_ranges.values()))\n",
    "    if write_ranges:\n",
    "        start_op_idx = min(write_ranges.keys())\n",
    "        end_op_idx = max(write_ranges.keys())\n",
    "        start_blob = min(elem[0] for elem in list(write_ranges.values())) #min(write_ranges.values())\n",
    "        end_blob = max(elem[1] for elem in list(write_ranges.values()))\n",
    "    \n",
    "    result = {\n",
    "        'start_op_idx': start_op_idx,\n",
    "        'end_op_idx': end_op_idx,\n",
    "        'start_blob': start_blob,\n",
    "        'end_blob': end_blob\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# TODO: need different get_min_max_op_idx\n",
    "def get_min_max_op_idx(io_stat):\n",
    "    # assumes we want the I/O operations and blobs for per file\n",
    "    min_op_idx = -1\n",
    "    max_op_idx = -1\n",
    "    min_blob = -1\n",
    "    max_blob = -1\n",
    "    for h5fd_type, h5fd_stat in io_stat.items():\n",
    "        res_stat = get_op_blob_range(h5fd_stat)\n",
    "        \n",
    "        if min_op_idx == -1:\n",
    "            min_op_idx = res_stat['start_op_idx']\n",
    "        elif res_stat['start_op_idx'] < min_op_idx:\n",
    "            min_op_idx = res_stat['start_op_idx']\n",
    "        \n",
    "        if max_op_idx == -1:\n",
    "            max_op_idx = res_stat['end_op_idx']\n",
    "        elif res_stat['end_op_idx'] > max_op_idx:\n",
    "            max_op_idx = res_stat['end_op_idx']\n",
    "        \n",
    "        if min_blob == -1:\n",
    "            min_blob = res_stat['start_blob']\n",
    "        elif res_stat['start_blob'] < min_blob:\n",
    "            min_blob = res_stat['start_blob']\n",
    "        \n",
    "        if max_blob == -1:\n",
    "            max_blob = res_stat['end_blob']\n",
    "        elif res_stat['end_blob'] > max_blob:\n",
    "            max_blob = res_stat['end_blob']\n",
    "    \n",
    "    result = {\n",
    "        'min_op_idx': min_op_idx,\n",
    "        'max_op_idx': max_op_idx,\n",
    "        'min_blob': min_blob,\n",
    "        'max_blob': max_blob\n",
    "    }\n",
    "    \n",
    "    # print(f\"h5fd_type: {h5fd_type}\")\n",
    "    # print(f\"res_stat: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Converting to Task-File dictionary\n",
    "def schema_to_task_file_dic(vfd_file_dict):\n",
    "    task_file_dict = {}\n",
    "    # Format of task_file_dict\n",
    "    '''\n",
    "    run_idfeature:\n",
    "        order: 0\n",
    "        task_pid: 75190\n",
    "        task_op_range: [0, 7208]\n",
    "        total_op_range: [0, 7208]\n",
    "        input:\n",
    "            - file_name: \"wrfout_rainrate_tb_zh_mh_2015-05-06_04:00:00.nc\"\n",
    "                file_op_range: [55, 71]\n",
    "        output:\n",
    "            - file_name: \"cloudid_20150506_040000.nc\"\n",
    "                ile_op_range: [0, 800]\n",
    "    '''\n",
    "    for file_path, file_data in vfd_file_dict.items():\n",
    "\n",
    "        task_pid = extract_pid_from_file_path(file_path)\n",
    "        \n",
    "        for li in file_data:\n",
    "            # print(li.keys())\n",
    "            if 'file' in list(li.keys())[0]:\n",
    "                key = list(li.keys())[0]\n",
    "                # print(li[key])\n",
    "                \n",
    "                if li[key]['file_type'] == 'na':\n",
    "                    print(\"Invalid Entry. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "                task_name = li[key]['task_name']\n",
    "                file_name = li[key]['file_name']\n",
    "\n",
    "                if task_name not in task_file_dict.keys():\n",
    "                    # 1. Initialize task_file_dict\n",
    "                    task_file_dict[task_name] = {}\n",
    "                    task_file_dict[task_name]['order'] = 0 # placeholder\n",
    "                    task_file_dict[task_name]['task_pid'] = task_pid\n",
    "                    task_file_dict[task_name]['io_cnt'] = 0 # initial value\n",
    "                    # task_file_dict[task_name]['total_op_range'] = []\n",
    "                    task_file_dict[task_name]['input'] = {}\n",
    "                    task_file_dict[task_name]['output'] = {}\n",
    "                    \n",
    "                # 1. Update task op count\n",
    "                read_cnt = li[key]['file_read_cnt']\n",
    "                write_cnt = li[key]['file_write_cnt']\n",
    "                task_file_dict[task_name]['io_cnt'] += (read_cnt + write_cnt)\n",
    "                \n",
    "                # 2. Get blob and operation info\n",
    "                \n",
    "                try:\n",
    "                    data_stat = li[key]['data']#['H5FD_MEM_DRAW']\n",
    "                except:\n",
    "                    data_stat = {}\n",
    "                    print(\"No data_stat with H5FD_MEM_DRAW...\")\n",
    "                try:\n",
    "                    meta_stats = li[key]['metadata']\n",
    "                except:\n",
    "                    meta_stats = {}\n",
    "                    print(\"No meta_stats ...\")\n",
    "                    \n",
    "                data_stat.update(meta_stats)\n",
    "\n",
    "                # try: \n",
    "                # except:\n",
    "                #     print(\"Error:\")\n",
    "                #     print(f\"task_name: {task_name}\")\n",
    "                #     print(f\"file_name: {file_name}\")\n",
    "                \n",
    "                if PREFETCH_SCHEMA:\n",
    "                    io_stat = get_min_max_op_idx(data_stat)\n",
    "                else:\n",
    "                    io_stat = {}\n",
    "                    \n",
    "                # 2. Add file to intput/output list\n",
    "                file_type = li[key]['file_type']\n",
    "                if file_type == 'input':\n",
    "                    # task_file_dict[task_name]['input'].append(file_name)\n",
    "                    task_file_dict[task_name]['input'][file_name] = io_stat\n",
    "                elif file_type == 'output':\n",
    "                    # task_file_dict[task_name]['output'].append(file_name)\n",
    "                    task_file_dict[task_name]['output'][file_name] = io_stat\n",
    "                elif file_type == 'input_output':\n",
    "                    # TODO: split read and write with detail in io_stat (according to operation order)\n",
    "                    # print(f\"{task_name} input_output io_stat: {io_stat}\")\n",
    "                    # task_file_dict[task_name]['input'][file_name] = io_stat\n",
    "                    task_file_dict[task_name]['output'][file_name] = io_stat\n",
    "                    # TODO: currently only doing output (as observed in DDMD read is after write)\n",
    "                \n",
    "\n",
    "                # print(f\"{file_name}: {io_stat}\")\n",
    "                # data_stat_range_list = list(data_stat['write_ranges'].values())\n",
    "                \n",
    "    return task_file_dict\n",
    "\n",
    "def sort_task_in_order(task_file_dict, task_order_list):\n",
    "    op_offset = 0\n",
    "    # for i, task_name in enumerate(task_order_list):\n",
    "    \n",
    "    for task_name_idx, stat in task_file_dict.items():\n",
    "        for task_name in task_order_list.keys():\n",
    "            if task_name in task_name_idx:\n",
    "                order = task_order_list[task_name]\n",
    "                task_file_dict[task_name_idx]['order'] = order\n",
    "                io_cnt = task_file_dict[task_name_idx]['io_cnt']\n",
    "                # task_file_dict[task_name]['total_op_range'] = f\"[{op_offset},{io_cnt}]\"\n",
    "                task_file_dict[task_name_idx]['total_op_range'] = {}\n",
    "                \n",
    "                task_file_dict[task_name_idx]['total_op_range']['start'] = op_offset\n",
    "                task_file_dict[task_name_idx]['total_op_range']['end'] = op_offset + io_cnt\n",
    "                \n",
    "                op_offset += io_cnt\n",
    "    \n",
    "    return task_file_dict\n",
    "    \n",
    "    # for task_name,i in task_order_list.items():\n",
    "    #     # if task_name in task_file_dict.keys():\n",
    "    #     task_file_dict[task_name]['order'] = i\n",
    "    #     io_cnt = task_file_dict[task_name]['io_cnt']\n",
    "    #     # task_file_dict[task_name]['total_op_range'] = f\"[{op_offset},{io_cnt}]\"\n",
    "    #     task_file_dict[task_name]['total_op_range'] = {}\n",
    "        \n",
    "    #     task_file_dict[task_name]['total_op_range']['start'] = op_offset\n",
    "    #     task_file_dict[task_name]['total_op_range']['end'] = op_offset + io_cnt\n",
    "\n",
    "    #     op_offset += io_cnt\n",
    "    # return task_file_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_pid: 170202\n",
      "Invalid Entry. Skipping...\n",
      "Invalid Entry. Skipping...\n",
      "Invalid Entry. Skipping...\n",
      "Invalid Entry. Skipping...\n",
      "Invalid Entry. Skipping...\n",
      "Invalid Entry. Skipping...\n",
      "Invalid Entry. Skipping...\n"
     ]
    }
   ],
   "source": [
    "# Save the task to input/output file mapping\n",
    "def save_task_file_dict(task_file_dict, test_name):\n",
    "    tf_file_path = f\"../example_stat/{test_name}/{test_name}-task_to_file.yaml\"\n",
    "    \n",
    "    # if os.path.exists(tf_file_path):\n",
    "    #     # Remove the file\n",
    "    #     os.remove(tf_file_path)\n",
    "    \n",
    "    with open(tf_file_path, 'w') as yaml_file:\n",
    "        for task_name,data in task_file_dict.items():\n",
    "            input_files = list(data['input'].keys())\n",
    "            output_files = list(data['output'].keys())\n",
    "            item_dict = {\n",
    "                task_name: {\n",
    "                    'order': data['order'],\n",
    "                    # 'task_pid': data['task_pid'], # TODO: not need?\n",
    "                    'io_cnt': data['io_cnt'], # TODO: not need?\n",
    "                    'input': input_files,\n",
    "                    'output': output_files\n",
    "                }\n",
    "            }\n",
    "            yaml.dump(item_dict, yaml_file)\n",
    "\n",
    "\n",
    "task_file_dict = schema_to_task_file_dic(vfd_file_dict)\n",
    "\n",
    "ordered_task_file_dict = sort_task_in_order(task_file_dict, TASK_ORDER_LIST)\n",
    "save_task_file_dict(task_file_dict, test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special code for task_file_dict\n",
    "def special_stat():\n",
    "    open_time = []\n",
    "    close_time = []\n",
    "\n",
    "    file_read_cnt = []\n",
    "    total_io_bytes = []\n",
    "\n",
    "    draw_rb = []\n",
    "    draw_rc= []\n",
    "\n",
    "    lheap_rb =[]\n",
    "    lheap_rc =[]\n",
    "    ohdr_rb = []\n",
    "    ohdr_rc =[]\n",
    "    super_rb = []\n",
    "    super_rc = []\n",
    "    btree_rb =[]\n",
    "    btree_rc =[]\n",
    "\n",
    "    all_file_stats = list(vfd_file_dict.values())[0]\n",
    "\n",
    "    # print(all_file_stats)\n",
    "\n",
    "    # for file_entry, stat in all_file_stats.items():\n",
    "    #     print(f\"{file_entry} : {stat}\")\n",
    "    for entry in all_file_stats:\n",
    "        for k, stat in entry.items():\n",
    "            # print(entry)\n",
    "            # stat = entry.values()\n",
    "            task = stat['task_name']\n",
    "            if task == 'train' and \"stage\" in stat['file_name'] and \"virtual\" not in stat['file_name']:\n",
    "                # print(stat)\n",
    "                if \"stage\" in stat['file_name']:\n",
    "                    open_time.append(stat['open_time'])\n",
    "                    close_time.append(stat['close_time'])\n",
    "                    file_read_cnt.append(stat['file_read_cnt'])\n",
    "                    total_io_bytes.append(stat['total_io_bytes'])\n",
    "                    draw = stat['data']['H5FD_MEM_DRAW']\n",
    "                    draw_rb.append(draw['read_bytes'])\n",
    "                    draw_rc.append(draw['read_cnt'])\n",
    "                    # print(stat['metadata'])\n",
    "                    lheap = stat['metadata']['H5FD_MEM_LHEAP']\n",
    "                    lheap_rb.append(lheap['read_bytes'])\n",
    "                    lheap_rc.append(lheap['read_cnt'])\n",
    "                    ohdr = stat['metadata']['H5FD_MEM_OHDR']\n",
    "                    ohdr_rb.append(ohdr['read_bytes'])\n",
    "                    ohdr_rc.append(ohdr['read_cnt'])\n",
    "                    super_mem = stat['metadata']['H5FD_MEM_SUPER']\n",
    "                    super_rb.append(super_mem['read_bytes'])\n",
    "                    super_rc.append(super_mem['read_cnt'])\n",
    "                    btree = stat['metadata']['H5FD_MEM_BTREE']\n",
    "                    btree_rb.append(btree['read_bytes'])\n",
    "                    btree_rc.append(btree['read_cnt'])\n",
    "\n",
    "    print(f\"open_time : {min(open_time)}\")\n",
    "    print(f\"close_time : {(min(open_time) + (1682060382292624 - 1682060382105156))}\")\n",
    "    print(f\"file_read_cnt : {sum(file_read_cnt)}\")\n",
    "    print(f\"total_io_bytes : {sum(total_io_bytes)}\")\n",
    "\n",
    "    print(f\"draw_rb : {sum(draw_rb)}\")\n",
    "    print(f\"draw_rc : {sum(draw_rc)}\")\n",
    "\n",
    "    print(f\"lheap_rb : {sum(lheap_rb)}\")\n",
    "    print(f\"lheap_rc : {sum(lheap_rc)}\")\n",
    "    print(f\"ohdr_rb : {sum(ohdr_rb)}\")\n",
    "    print(f\"ohdr_rc : {sum(ohdr_rc)}\")\n",
    "    print(f\"super_rb : {sum(super_rb)}\")\n",
    "    print(f\"super_rc : {sum(super_rc)}\")\n",
    "    print(f\"btree_rb : {sum(btree_rb)}\")\n",
    "    print(f\"btree_rc : {sum(btree_rc)}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARALLEL=True # True False\n",
    "\n",
    "\n",
    "# Temporary\n",
    "# RUN_NAME=\"wrf_tbradar_hm\"\n",
    "RUN_NAME=\"olr_pcp\"\n",
    "\n",
    "FILE_PATH_KEY = {\n",
    "    \"wrfout_rainrate_\" : f\"/qfs/projects/oddite/tang584/flextrkr_runs/input_data/{RUN_NAME}\",\n",
    "    \"wrf_\" : f\"/qfs/projects/oddite/tang584/flextrkr_runs/input_data/{RUN_NAME}\",\n",
    "    \"cloudid_\" : f\"/qfs/projects/oddite/tang584/flextrkr_runs/{RUN_NAME}/tracking\",\n",
    "    \"tracknumbers_\" : f\"/qfs/projects/oddite/tang584/flextrkr_runs/{RUN_NAME}/stats\",\n",
    "    \"trackstats_sparse_\" : f\"/qfs/projects/oddite/tang584/flextrkr_runs/{RUN_NAME}/stats\",\n",
    "    \"mcs_tracks_\" : f\"/qfs/projects/oddite/tang584/flextrkr_runs/{RUN_NAME}/stats\",\n",
    "    # \"mcstrack_\" : f\"/qfs/projects/oddite/tang584/flextrkr_runs/{RUN_NAME}/mcstracking/20150506.0000_20150506.0800\",\n",
    "    \"mcstrack_\" : f\"/qfs/projects/oddite/tang584/flextrkr_runs/{RUN_NAME}/mcstracking/20160801.0000_20160801.0500\",\n",
    "    \"pr_rlut_mean_sam_\" : f\"/qfs/projects/oddite/tang584/flextrkr_runs/input_data/{RUN_NAME}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../example_stat/summer_sam_seq12f9s/apriori_summer_sam_seq12f9s.yaml\n"
     ]
    }
   ],
   "source": [
    "# Convert dictionary to prefetcher format\n",
    "def save_hermes_prefetch(task_file_dict, test_name):\n",
    "\n",
    "    prefetch_file_path = f\"../example_stat/{test_name}/apriori_{test_name}.yaml\"\n",
    "\n",
    "    with open(prefetch_file_path, 'w') as file:\n",
    "        file.write(\"0:\\n\")\n",
    "        # for task_name,data in task_file_dict.items():\n",
    "        for data in task_file_dict.values():\n",
    "            if PARALLEL:\n",
    "                # TODO: add only the earliest op_range for multi-process runs\n",
    "                op_range = \"\"\n",
    "                for input_file,file_stat in data['input'].items():\n",
    "                    if op_range == \"\":\n",
    "                        # op_range = f\"[{file_stat['min_op_idx']},{file_stat['max_op_idx']}]\"\n",
    "                        # op_range = f\"[{file_stat['max_op_idx']-20},{file_stat['max_op_idx']}]\"\n",
    "                        # op_range = f\"[{file_stat['min_op_idx']},{file_stat['min_op_idx']+80}]\"\n",
    "                        op_range = f\"[{file_stat['min_op_idx']},{file_stat['min_op_idx']+20}]\"\n",
    "                        \n",
    "                    if file_stat['max_blob'] <= 1:\n",
    "                        blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "                    else:\n",
    "                        blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']-1}]\"\n",
    "                    \n",
    "                    # blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "                    input_file_path = input_file\n",
    "                    # for key,val in FILE_PATH_KEY.items():\n",
    "                    #     if key in input_file:\n",
    "                    #         input_file_path = f\"{val}/{input_file}\"\n",
    "                    file.write(f\"  - bucket: {input_file_path}\\n\")\n",
    "                    file.write(f\"    prefetch:\\n\")\n",
    "                    file.write(f\"    - op_count_range: {op_range}\\n\")\n",
    "                    file.write(f\"      promote_blobs: {blob_range}\\n\")\n",
    "            else:\n",
    "                for input_file,file_stat in data['input'].items():\n",
    "                    op_range = f\"[{file_stat['min_op_idx']},{file_stat['max_op_idx']}]\"\n",
    "                    blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "                    input_file_path = input_file\n",
    "                    # for key,val in FILE_PATH_KEY.items():\n",
    "                    #     if key in input_file:\n",
    "                    #         input_file_path = f\"{val}/{input_file}\"\n",
    "                    file.write(f\"  - bucket: {input_file_path}\\n\")\n",
    "                    file.write(f\"    prefetch:\\n\")\n",
    "                    file.write(f\"    - op_count_range: {op_range}\\n\")\n",
    "                    file.write(f\"      promote_blobs: {blob_range}\\n\")\n",
    "            \n",
    "            # TODO: only demote blobs not used by later stage\n",
    "            # for output_file,file_stat in data['input'].items():\n",
    "            #     op_range = f\"[{file_stat['min_op_idx']},{file_stat['max_op_idx']}]\"\n",
    "            #     blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "            #     file.write(f\"  - bucket: {output_file}\\n\")\n",
    "            #     file.write(f\"    prefetch:\\n\")\n",
    "            #     file.write(f\"    - op_count_range: {op_range}\\n\")\n",
    "            #     file.write(f\"      demote_blobs: {blob_range}\\n\")\n",
    "            \n",
    "            \n",
    "            # print(input_file)\n",
    "            # print(file_stat)\n",
    "    print(f\"Saved to {prefetch_file_path}\")\n",
    "    \n",
    "if PREFETCH_SCHEMA:\n",
    "    save_hermes_prefetch(task_file_dict, test_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../example_stat/summer_sam_seq12f9s/apriori_summer_sam_seq12f9s_compact.yaml\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def extract_hermes_prefetch(task_file_dict):\n",
    "    pf_dict = {}\n",
    "    # TODO: current only 1 thread\n",
    "    \n",
    "    \n",
    "    for data in task_file_dict.values():\n",
    "        if TRACE_PARALLEL:\n",
    "            # TODO: add only the earliest op_range for multi-process runs\n",
    "            op_range = \"\"\n",
    "            for input_file,file_stat in data['input'].items():\n",
    "                if op_range == \"\":\n",
    "                    # op_range = f\"[{file_stat['min_op_idx']},{file_stat['max_op_idx']}]\"\n",
    "                    # op_range = f\"[{file_stat['max_op_idx']-20},{file_stat['max_op_idx']}]\"\n",
    "                    # op_range = f\"[{file_stat['min_op_idx']},{file_stat['min_op_idx']+80}]\"\n",
    "                    op_range = f\"[{file_stat['min_op_idx']},{file_stat['min_op_idx']+20}]\"\n",
    "                    \n",
    "                if file_stat['max_blob'] <= 1:\n",
    "                    blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "                else:\n",
    "                    blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']-1}]\"\n",
    "                \n",
    "                input_file_path = input_file\n",
    "                \n",
    "                if input_file_path not in pf_dict.keys():\n",
    "                    pf_dict[input_file_path] = {}\n",
    "                    pf_dict[input_file_path]['prefetch'] = []\n",
    "                \n",
    "                prefetch_entry = {\n",
    "                    'op_count_range': op_range,\n",
    "                    'promote_blobs': blob_range\n",
    "                }\n",
    "                \n",
    "                pf_dict[input_file_path]['prefetch'].append(prefetch_entry)\n",
    "                \n",
    "        else:\n",
    "            op_range = \"\"\n",
    "            for input_file,file_stat in data['input'].items():\n",
    "                if op_range == \"\":\n",
    "                    op_range_start = file_stat['min_op_idx'] #math.floor(file_stat['min_op_idx']/PROCS)\n",
    "                    if op_range_start > 100:\n",
    "                        op_range_start = op_range_start - 100\n",
    "                    op_range_end = op_range_start + 20\n",
    "                    op_range = f\"[{op_range_start},{op_range_end}]\"\n",
    "                \n",
    "                blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "                input_file_path = input_file\n",
    "\n",
    "                if input_file_path not in pf_dict.keys():\n",
    "                    pf_dict[input_file_path] = {}\n",
    "                    pf_dict[input_file_path]['prefetch'] = []\n",
    "                \n",
    "                prefetch_entry = {\n",
    "                    'op_count_range': op_range,\n",
    "                    'promote_blobs': blob_range\n",
    "                }\n",
    "                \n",
    "                pf_dict[input_file_path]['prefetch'].append(prefetch_entry)\n",
    "    return pf_dict\n",
    "\n",
    "def save_prefetch_to_file(pf_dict, test_name):\n",
    "    \n",
    "    prefetch_file_path = f\"../example_stat/{test_name}/apriori_{test_name}_compact.yaml\"\n",
    "    \n",
    "    with open(prefetch_file_path, 'w') as file:\n",
    "        file.write(\"0:\\n\")\n",
    "        # for task_name,data in task_file_dict.items():\n",
    "        for key,val in pf_dict.items():\n",
    "            \n",
    "            # file.write(f\"  - bucket: {key}\\n\")\n",
    "            # file.write(f\"    prefetch:\\n\")\n",
    "            # for i,iv in enumerate(val['prefetch']):\n",
    "            #     file.write(f\"    - op_count_range: {iv['op_count_range']}\\n\")\n",
    "            #     file.write(f\"      promote_blobs: {iv['promote_blobs']}\\n\")\n",
    "                \n",
    "            first_access = val['prefetch'][0]\n",
    "            if \"[0,\" in first_access['op_count_range']:\n",
    "                # Prefetch initial input as stage-in\n",
    "                file.write(f\"  - bucket: {key}\\n\")\n",
    "                file.write(f\"    prefetch:\\n\")\n",
    "                for i,iv in enumerate(val['prefetch']):\n",
    "                    op_count_range = iv['op_count_range']\n",
    "                    op_count_range = op_count_range.replace(\"[0,\", \"[1,\")\n",
    "                    file.write(f\"    - op_count_range: {op_count_range}\\n\")\n",
    "                    file.write(f\"      promote_blobs: {iv['promote_blobs']}\\n\")\n",
    "            elif len(val['prefetch']) > 1:\n",
    "                # prefetch intermediate files when it's not written immediately after\n",
    "                file.write(f\"  - bucket: {key}\\n\")\n",
    "                file.write(f\"    prefetch:\\n\")\n",
    "                # file.write(f\"    - op_count_range: {iv['op_count_range']}\\n\")\n",
    "                # file.write(f\"      promote_blobs: {iv['promote_blobs']}\\n\")\n",
    "                for i,iv in enumerate(val['prefetch']):\n",
    "                    if TRACE_PARALLEL:\n",
    "                        file.write(f\"    - op_count_range: {iv['op_count_range']}\\n\")\n",
    "                        file.write(f\"      promote_blobs: {iv['promote_blobs']}\\n\")\n",
    "                    else:\n",
    "                        if i != 0:\n",
    "                            file.write(f\"    - op_count_range: {iv['op_count_range']}\\n\")\n",
    "                            file.write(f\"      promote_blobs: {iv['promote_blobs']}\\n\")\n",
    "    \n",
    "    print(f\"Saved to {prefetch_file_path}\")\n",
    "\n",
    "if PREFETCH_SCHEMA:\n",
    "    pf_dict = extract_hermes_prefetch(task_file_dict)\n",
    "    save_prefetch_to_file(pf_dict, test_name)\n",
    "\n",
    "\n",
    "# print(pf_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
